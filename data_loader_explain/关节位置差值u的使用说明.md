# å…³èŠ‚ä½ç½®å·®å€¼ u çš„ä½¿ç”¨è¯´æ˜

## æ¦‚è¿°

ç°åœ¨è½¬æ¢åçš„transforms.jsonæ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªframeéƒ½åŒ…å«äº†å…³èŠ‚ä½ç½®å·®å€¼ `u`ï¼Œè®¡ç®—æ–¹å¼ä¸ºï¼š

```
u = current_frame.joint_pos - previous_frame.joint_pos
```

è¿™ä¸ªå·®å€¼å¯ä»¥ä½œä¸ºè¾“å…¥æ§åˆ¶ä¿¡å·ç”¨äºæ¡ä»¶åŒ–çš„4Dé«˜æ–¯æº…å°„è®­ç»ƒã€‚

## æ•°æ®æ ¼å¼

### ç”Ÿæˆçš„transforms.jsonæ ¼å¼

```json
{
    "camera_angle_x": 0.8575560450553894,
    "fl_x": 606.572265625,
    "fl_y": 605.9955444335938,
    "cx": 327.9910583496094,
    "cy": 239.932373046875,
    "w": 640,
    "h": 480,
    "frames": [
        {
            "file_path": "view_0/rgb/00000_00000.png",
            "transform_matrix": [[...], [...], [...], [...]],
            "time": 0.0,
            "u": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  // ç¬¬ä¸€å¸§ï¼Œå·®å€¼ä¸º0
            "joint_pos": [-6.3, -9.82, -8.65, -11.0, 1.03, 6.01],  // è°ƒè¯•ç”¨
            "sample_idx": 0,
            "camera_idx": 0
        },
        {
            "file_path": "view_1/rgb/00000_00000.png",
            "transform_matrix": [[...], [...], [...], [...]],
            "time": 0.0,
            "u": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  // åŒä¸€æ—¶é—´æ­¥çš„å¦ä¸€è§†è§’ï¼Œä¹Ÿæ˜¯ç¬¬ä¸€å¸§
            "joint_pos": [-6.3, -9.82, -8.65, -11.0, 1.03, 6.01],
            "sample_idx": 0,
            "camera_idx": 1
        },
        {
            "file_path": "view_0/rgb/00000_00001.png",
            "transform_matrix": [[...], [...], [...], [...]],
            "time": 0.111,
            "u": [2.45, 1.23, -0.56, 0.89, -0.12, 0.34],  // ç›¸å¯¹äºä¸Šä¸€æ—¶é—´æ­¥çš„å·®å€¼
            "joint_pos": [-3.85, -8.59, -9.21, -10.11, 0.91, 6.35],
            "sample_idx": 1,
            "camera_idx": 0
        }
    ]
}
```

### å­—æ®µè¯´æ˜

- **`u`**: å…³èŠ‚ä½ç½®å·®å€¼ï¼Œnumpyæ•°ç»„ï¼Œç»´åº¦ç­‰äºå…³èŠ‚æ•°ï¼ˆToyArmæ˜¯6ï¼‰
  - ç¬¬ä¸€å¸§ï¼šu = [0, 0, 0, 0, 0, 0]
  - åç»­å¸§ï¼šu = å½“å‰å¸§joint_pos - åŒä¸€ç›¸æœºå‰ä¸€å¸§çš„joint_pos
  
- **`joint_pos`**: åŸå§‹å…³èŠ‚ä½ç½®ï¼Œä¿ç•™ç”¨äºè°ƒè¯•å’ŒéªŒè¯

- **`sample_idx`**: æ—¶é—´æ­¥ç´¢å¼•

- **`camera_idx`**: ç›¸æœºç´¢å¼•ï¼ˆ0-11ï¼‰

## è®¡ç®—é€»è¾‘

### æ–¹æ¡ˆä¸€ï¼šè½¬æ¢è„šæœ¬

è½¬æ¢è„šæœ¬ä¼šæŒ‰ä»¥ä¸‹é€»è¾‘è®¡ç®—uï¼š

1. **æŒ‰æ—¶é—´å’Œç›¸æœºæ’åº**ï¼šframesæŒ‰(time, camera_idx)æ’åº
2. **ä¸ºæ¯ä¸ªç›¸æœºç‹¬ç«‹è®¡ç®—**ï¼šæ¯ä¸ªç›¸æœºç»´æŠ¤è‡ªå·±çš„prev_joint_pos
3. **è®¡ç®—å·®å€¼**ï¼š
   - ç¬¬ä¸€å¸§ï¼ˆæ¯ä¸ªç›¸æœºï¼‰ï¼šu = zeros
   - åç»­å¸§ï¼šu = current_joint_pos - prev_joint_pos

```python
# ä¼ªä»£ç 
prev_joint_pos_per_camera = {}

for frame in sorted_frames:
    camera_idx = frame['camera_idx']
    curr_joint_pos = frame['joint_pos']
    
    if camera_idx in prev_joint_pos_per_camera:
        u = curr_joint_pos - prev_joint_pos_per_camera[camera_idx]
    else:
        u = zeros  # ç¬¬ä¸€å¸§
    
    frame['u'] = u
    prev_joint_pos_per_camera[camera_idx] = curr_joint_pos
```

### æ–¹æ¡ˆäºŒï¼šæ•°æ®åŠ è½½å™¨

NJFæ•°æ®åŠ è½½å™¨åœ¨è¿è¡Œæ—¶åŠ¨æ€è®¡ç®—uï¼Œé€»è¾‘ç›¸åŒã€‚

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨è½¬æ¢è„šæœ¬

```bash
# è½¬æ¢æ•°æ®ï¼ˆè‡ªåŠ¨è®¡ç®—uï¼‰
python convert_njf_to_4dgs.py \
    --input d:\Codee\neural-jacobian-field\data\toyarm\transforms.json \
    --output d:\Codee\4DGaussians\data\toyarm_with_u

# æ£€æŸ¥ç”Ÿæˆçš„u
python -c "
import json
data = json.load(open('d:/Codee/4DGaussians/data/toyarm_with_u/transforms_train.json'))
print('First frame u:', data['frames'][0]['u'])
print('Second frame u:', data['frames'][12]['u'])  # ç¬¬äºŒæ—¶é—´æ­¥ï¼Œç¬¬ä¸€ä¸ªç›¸æœº
"

# åˆ›å»ºç¬¦å·é“¾æ¥
cd d:\Codee\4DGaussians\data\toyarm_with_u
0..11 | ForEach-Object { 
    cmd /c mklink /J "view_$_" "d:\Codee\neural-jacobian-field\data\toyarm\view_$_" 
}

# è®­ç»ƒï¼ˆåœ¨ä½ çš„è®­ç»ƒä»£ç ä¸­ä½¿ç”¨uï¼‰
python train.py -s data\toyarm_with_u -m output\toyarm --eval
```

### æ–¹æ¡ˆäºŒï¼šä½¿ç”¨æ•°æ®åŠ è½½å™¨

```bash
# ä¿®æ”¹dataset_readers.pyåï¼Œç›´æ¥è®­ç»ƒ
python train.py \
    -s d:\Codee\neural-jacobian-field\data\toyarm \
    --dataset_type NJF \
    -m output\toyarm \
    --eval
```

æ•°æ®åŠ è½½å™¨ä¼šåœ¨è¿è¡Œæ—¶è®¡ç®—uå¹¶æ·»åŠ åˆ°CameraInfoä¸­ã€‚

## åœ¨4DGaussiansä¸­ä½¿ç”¨u

### 1. ä¿®æ”¹Cameraç±»ä»¥åŒ…å«u

ç¼–è¾‘ `scene/cameras.py`ï¼š

```python
class Camera(nn.Module):
    def __init__(self, colmap_id, R, T, FoVx, FoVy, image, gt_alpha_mask,
                 image_name, uid,
                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = "cuda",
                 time=0.0, u=None  # æ·»åŠ uå‚æ•°
                 ):
        super(Camera, self).__init__()
        
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
        
        self.time = time
        self.u = torch.tensor(u, dtype=torch.float32) if u is not None else None  # æ·»åŠ uå±æ€§
```

### 2. ä¿®æ”¹loadCamå‡½æ•°

ç¼–è¾‘ `utils/camera_utils.py`ï¼š

```python
def loadCam(args, id, cam_info, resolution_scale):
    # ... å…¶ä»–ä»£ç  ...
    
    # æå–uï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    u = None
    if hasattr(cam_info, 'u'):
        u = cam_info.u
    elif hasattr(cam_info, 'joint_pos'):
        # å¦‚æœæ²¡æœ‰uä½†æœ‰joint_posï¼Œå¯ä»¥åœ¨è¿™é‡Œè®¡ç®—
        pass
    
    return Camera(
        colmap_id=cam_info.uid, R=cam_info.R, T=cam_info.T, 
        FoVx=cam_info.FovX, FoVy=cam_info.FovY, 
        image=cam_info.image, gt_alpha_mask=None,
        image_name=cam_info.image_name, uid=id, data_device=args.data_device, 
        time=cam_info.time,
        u=u  # ä¼ é€’u
    )
```

### 3. åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨u

ç¼–è¾‘ `train.py`ï¼š

```python
def training(dataset, opt, pipe, testing_iterations, saving_iterations, ...):
    # ... è®­ç»ƒå¾ªç¯ ...
    
    for iteration in range(first_iter, opt.iterations + 1):
        # è·å–å½“å‰camera
        viewpoint_cam = scene.getTrainCameras()[viewpoint_index]
        
        # è·å–uï¼ˆå…³èŠ‚ä½ç½®å·®å€¼ï¼‰
        if viewpoint_cam.u is not None:
            u = viewpoint_cam.u.to(device)
            
            # å°†uä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°æ¨¡å‹
            # ä¾‹å¦‚ï¼šæ¡ä»¶åŒ–deformation network
            render_pkg = render(viewpoint_cam, gaussians, pipe, background, 
                               control_signal=u)  # æ·»åŠ æ§åˆ¶ä¿¡å·
        else:
            render_pkg = render(viewpoint_cam, gaussians, pipe, background)
```

### 4. æ¡ä»¶åŒ–Deformation Networkï¼ˆç¤ºä¾‹ï¼‰

å¦‚æœä½ æƒ³ç”¨uæ¥æ§åˆ¶å˜å½¢ï¼š

```python
class DeformationNetwork(nn.Module):
    def __init__(self, D=8, W=256, input_ch=3, input_ch_time=1, 
                 input_ch_control=6):  # æ·»åŠ æ§åˆ¶ç»´åº¦
        super(DeformationNetwork, self).__init__()
        
        self.control_encoder = nn.Sequential(
            nn.Linear(input_ch_control, 64),
            nn.ReLU(),
            nn.Linear(64, 128)
        )
        
        # ... å…¶ä»–ç½‘ç»œç»“æ„ ...
    
    def forward(self, x, t, u=None):
        # x: 3Dä½ç½®
        # t: æ—¶é—´
        # u: å…³èŠ‚ä½ç½®å·®å€¼ï¼ˆæ§åˆ¶ä¿¡å·ï¼‰
        
        if u is not None:
            control_feat = self.control_encoder(u)
            # å°†control_featèåˆåˆ°å˜å½¢ç½‘ç»œä¸­
            # ...
        
        # è¿”å›å˜å½¢åçš„ä½ç½®
        return deformed_x
```

## ç»Ÿè®¡ä¿¡æ¯

è½¬æ¢å®Œæˆåä¼šæ˜¾ç¤ºuçš„ç»Ÿè®¡ä¿¡æ¯ï¼š

```
âœ… Joint position differences (u) computed successfully!
  - u dimension: 6
  - Example u: ['2.450', '1.230', '-0.560', '0.890', '-0.120', '0.340']
  - u statistics:
    Mean: [ 0.123 -0.045  0.089  0.034 -0.012  0.067]
    Std:  [ 2.341  1.890  1.456  2.123  0.890  1.234]
    Max:  [ 8.920  7.650  6.430  9.120  3.450  5.670]
```

è¿™äº›ç»Ÿè®¡å¯ä»¥å¸®åŠ©ä½ ï¼š
- äº†è§£å…³èŠ‚è¿åŠ¨çš„å¹…åº¦
- å½’ä¸€åŒ–uï¼ˆå¦‚æœéœ€è¦ï¼‰
- æ£€æŸ¥æ•°æ®æ˜¯å¦åˆç†

## éªŒè¯uçš„æ­£ç¡®æ€§

```python
import json
import numpy as np

# åŠ è½½æ•°æ®
data = json.load(open('data/toyarm_with_u/transforms_train.json'))

# æŒ‰camera_idxåˆ†ç»„
frames_by_camera = {}
for frame in data['frames']:
    cam_idx = frame['camera_idx']
    if cam_idx not in frames_by_camera:
        frames_by_camera[cam_idx] = []
    frames_by_camera[cam_idx].append(frame)

# å¯¹æ¯ä¸ªç›¸æœºï¼ŒæŒ‰æ—¶é—´æ’åºå¹¶éªŒè¯u
for cam_idx, frames in frames_by_camera.items():
    frames_sorted = sorted(frames, key=lambda x: x['time'])
    
    print(f"\nCamera {cam_idx}:")
    for i in range(len(frames_sorted)):
        frame = frames_sorted[i]
        u = np.array(frame['u'])
        joint_pos = np.array(frame['joint_pos'])
        
        if i == 0:
            # ç¬¬ä¸€å¸§ï¼Œuåº”è¯¥ä¸º0
            assert np.allclose(u, 0), f"First frame u should be zero, got {u}"
            print(f"  Frame {i}: u = {u} (first frame, correct)")
        else:
            # éªŒè¯ u = current - previous
            prev_joint_pos = np.array(frames_sorted[i-1]['joint_pos'])
            expected_u = joint_pos - prev_joint_pos
            assert np.allclose(u, expected_u), f"u mismatch at frame {i}"
            print(f"  Frame {i}: u = {u} (verified)")

print("\nâœ… All u values verified successfully!")
```

## é«˜çº§ç”¨æ³•

### å½’ä¸€åŒ–u

å¦‚æœuçš„èŒƒå›´å¤ªå¤§ï¼Œå¯ä»¥å½’ä¸€åŒ–ï¼š

```python
import json
import numpy as np

# åŠ è½½å¹¶è®¡ç®—ç»Ÿè®¡
data = json.load(open('transforms_train.json'))
all_u = np.array([f['u'] for f in data['frames']])

mean_u = np.mean(all_u, axis=0)
std_u = np.std(all_u, axis=0)

# å½’ä¸€åŒ–
for frame in data['frames']:
    frame['u_normalized'] = ((np.array(frame['u']) - mean_u) / std_u).tolist()

# ä¿å­˜
json.dump(data, open('transforms_train_normalized.json', 'w'), indent=4)
```

### æ—¶é—´å¹³æ»‘

å¦‚æœuæŠ–åŠ¨å¤ªå¤§ï¼Œå¯ä»¥å¹³æ»‘ï¼š

```python
from scipy.ndimage import gaussian_filter1d

# æŒ‰cameraåˆ†ç»„å¹¶å¹³æ»‘
for cam_idx in range(12):
    cam_frames = [f for f in data['frames'] if f['camera_idx'] == cam_idx]
    cam_frames_sorted = sorted(cam_frames, key=lambda x: x['time'])
    
    u_array = np.array([f['u'] for f in cam_frames_sorted])
    u_smoothed = gaussian_filter1d(u_array, sigma=1, axis=0)
    
    for frame, u_smooth in zip(cam_frames_sorted, u_smoothed):
        frame['u_smoothed'] = u_smooth.tolist()
```

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆç¬¬ä¸€å¸§çš„uæ˜¯0ï¼Ÿ

å› ä¸ºæ²¡æœ‰"å‰ä¸€å¸§"æ¥è®¡ç®—å·®å€¼ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºç¬¬ä¸€å¸§å¯ä»¥çœ‹ä½œæ˜¯ä»é™æ­¢çŠ¶æ€å¼€å§‹ã€‚

### Q2: ä¸åŒç›¸æœºçš„ç¬¬ä¸€å¸§uéƒ½æ˜¯0å—ï¼Ÿ

æ˜¯çš„ã€‚æ¯ä¸ªç›¸æœºç‹¬ç«‹è®¡ç®—uï¼Œæ‰€ä»¥æ¯ä¸ªç›¸æœºçš„ç¬¬ä¸€å¸§uéƒ½æ˜¯0ã€‚

### Q3: uçš„ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ

ç­‰äºå…³èŠ‚æ•°ã€‚ToyArmæœ‰6ä¸ªå…³èŠ‚ï¼Œæ‰€ä»¥uçš„ç»´åº¦æ˜¯6ã€‚

### Q4: å¦‚ä½•åœ¨è®­ç»ƒä¸­ä½¿ç”¨uï¼Ÿ

æœ‰å¤šç§æ–¹å¼ï¼š
1. ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°deformation network
2. ç”¨äºè°ƒåˆ¶feature
3. ä½œä¸ºé¢å¤–çš„embedding
4. ç”¨äºç‰©ç†çº¦æŸ

å…·ä½“å–å†³äºä½ çš„æ¨¡å‹è®¾è®¡ã€‚

### Q5: uå’Œtimeçš„åŒºåˆ«ï¼Ÿ

- `time`: è¡¨ç¤ºæ—¶é—´æˆ³ï¼ˆ0-1ï¼‰ï¼Œç”¨äºtemporal modeling
- `u`: è¡¨ç¤ºæ§åˆ¶ä¿¡å·ï¼ˆå…³èŠ‚è¿åŠ¨ï¼‰ï¼Œç”¨äºæ§åˆ¶åœºæ™¯å˜åŒ–

ä¸¤è€…äº’è¡¥ï¼Œtimeå‘Šè¯‰æ¨¡å‹"ä½•æ—¶"ï¼Œuå‘Šè¯‰æ¨¡å‹"å¦‚ä½•è¿åŠ¨"ã€‚

## å‚è€ƒèµ„æ–™

- åŸå§‹NJFæ•°æ®æ ¼å¼ï¼š`neural-jacobian-field/notebooks/real_world/dataset_configs/toy_arm_config.json`
- è½¬æ¢è„šæœ¬ï¼š`convert_njf_to_4dgs.py`
- æ•°æ®åŠ è½½å™¨ï¼š`scene/njf_toyarm_loader.py`

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜è¯·å‚è€ƒå…¶ä»–æ–‡æ¡£æˆ–æé—®ã€‚** ğŸš€
